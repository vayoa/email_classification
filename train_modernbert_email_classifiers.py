#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Train ModernBERT on email classification for (1) category (multiclass) and (2) important (binary).

Expected CSV columns: subject, body, text, category, important
- 'text' should already be subject+body; if missing, we fall back to subject + "\\n\\n" + body.
- 'category' is a string label (e.g., Forum, Promotions, Social Media, Spam, Updates, Verify Code)
- 'important' is 0/1 (or boolean) generated by your rule-based system.

Usage:
    python train_modernbert_email_classifiers.py --csv data/emails.csv --text-col text --epochs 3

Outputs:
    - Two fine-tuned models saved under outputs/modernbert_category and outputs/modernbert_important
    - Evaluation metrics JSON for each task.
"""
import argparse
import json
import os
import random
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
import torch
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_recall_fscore_support,
    roc_auc_score,
)
from sklearn.model_selection import train_test_split
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
    set_seed,
)

# -----------------------------
# Utils
# -----------------------------


def infer_device_dtype():
    if torch.cuda.is_available():
        return "cuda", (
            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        )
    if torch.backends.mps.is_available():
        return "mps", None
    return "cpu", None


def prepare_text_column(df: pd.DataFrame, text_col: Optional[str]) -> pd.Series:
    if text_col and text_col in df.columns:
        return df[text_col].astype(str)
    # Fallback: subject + body
    subj = df["subject"].fillna("").astype(str) if "subject" in df.columns else ""
    body = df["body"].fillna("").astype(str) if "body" in df.columns else ""
    combined = (subj + "\\n\\n" + body).str.strip()
    return combined


def clean_labels(df: pd.DataFrame, label_col: str) -> pd.Series:
    if label_col not in df.columns:
        raise ValueError(f"Label column '{label_col}' not found in CSV.")
    s = df[label_col]
    if label_col == "important":
        # Coerce to 0/1 integers
        s = s.map(
            lambda x: (
                1 if str(x).strip().lower() in {"1", "true", "yes", "y", "t"} else 0
            )
        ).astype(int)
    else:
        s = s.astype(str)
    return s


def compute_class_weights(labels: List[int], num_labels: int) -> torch.Tensor:
    # Inverse frequency weights
    counts = np.bincount(labels, minlength=num_labels)
    counts = np.where(counts == 0, 1, counts)  # avoid div by zero
    weights = 1.0 / counts
    weights = weights * (num_labels / weights.sum())
    return torch.tensor(weights, dtype=torch.float)


# -----------------------------
# Dataset wrapper
# -----------------------------


class SimpleTextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings: Dict[str, torch.Tensor], labels: List[int]):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: v[idx] for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


# -----------------------------
# Weighted loss Trainer
# -----------------------------


class WeightedTrainer(Trainer):
    def __init__(self, *args, class_weights: Optional[torch.Tensor] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # Accept extra kwargs (e.g., num_items_in_batch) from newer Trainer versions
        labels = inputs.get("labels")
        outputs = model(**{k: v for k, v in inputs.items() if k != "labels"})
        logits = outputs.get("logits")
        cw = (
            self.class_weights.to(logits.device)
            if self.class_weights is not None
            else None
        )
        loss_fct = torch.nn.CrossEntropyLoss(weight=cw)
        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
        return (loss, outputs) if return_outputs else loss


# -----------------------------
# Metrics
# -----------------------------


def multiclass_metrics(eval_pred, id2label: Dict[int, str]):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    p, r, f1, _ = precision_recall_fscore_support(
        labels, preds, average="macro", zero_division=0
    )
    wf1 = f1_score(labels, preds, average="weighted", zero_division=0)
    return {
        "accuracy": acc,
        "macro_f1": f1,
        "weighted_f1": wf1,
        "macro_precision": p,
        "macro_recall": r,
    }


def binary_metrics(eval_pred):
    logits, labels = eval_pred
    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    p, r, f1, _ = precision_recall_fscore_support(
        labels, preds, average="binary", zero_division=0
    )
    try:
        auc = roc_auc_score(labels, probs[:, 1])
    except Exception:
        auc = float("nan")
    return {"accuracy": acc, "precision": p, "recall": r, "f1": f1, "roc_auc": auc}


# -----------------------------
# Tokenize
# -----------------------------


def tokenize_texts(tokenizer, texts: List[str], max_length: int, padding: str):
    return tokenizer(
        texts,
        truncation=True,
        max_length=max_length,
        padding=padding,  # 'longest' for dynamic, or 'max_length'
        return_tensors="pt",
    )


# -----------------------------
# Training loop for one task
# -----------------------------


def train_one_task(
    model_name: str,
    train_texts: List[str],
    train_labels: List[int],
    val_texts: List[str],
    val_labels: List[int],
    output_dir: str,
    task_name: str,
    id2label: Dict[int, str],
    label2id: Dict[str, int],
    max_length: int = 1024,
    lr: float = 2e-5,
    batch_size: int = 8,
    grad_accum: int = 4,
    epochs: int = 3,
    use_weighted_loss: bool = True,
    evaluation_strategy: str = "epoch",
    save_strategy: str = "epoch",
    seed: int = 42,
):

    set_seed(seed)

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

    # ModernBERT doesn't use token_type_ids; AutoTokenizer will handle this.

    padding = "longest"  # more memory-efficient than static padding
    enc_train = tokenize_texts(
        tokenizer, train_texts, max_length=max_length, padding=padding
    )
    enc_val = tokenize_texts(
        tokenizer, val_texts, max_length=max_length, padding=padding
    )

    train_ds = SimpleTextDataset(enc_train, train_labels)
    val_ds = SimpleTextDataset(enc_val, val_labels)

    num_labels = len(id2label)
    config = AutoConfig.from_pretrained(
        model_name,
        num_labels=num_labels,
        id2label={int(k): v for k, v in id2label.items()},
        label2id=label2id,
    )
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, config=config
    )

    # Enable gradient checkpointing to reduce memory
    if hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()

    device, dtype = infer_device_dtype()
    if device == "cuda":
        model = model.to("cuda")
        if dtype is not None:
            model = model.to(dtype=dtype)
    elif device == "mps":
        model = model.to("mps")

    # Class weights for imbalance
    class_weights = None
    if use_weighted_loss:
        class_weights = compute_class_weights(train_labels, num_labels)

    args = TrainingArguments(
        output_dir=output_dir,
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=max(1, batch_size // 2),
        gradient_accumulation_steps=grad_accum,
        num_train_epochs=epochs,
        eval_strategy=evaluation_strategy,
        save_strategy=save_strategy,
        load_best_model_at_end=True,
        metric_for_best_model="macro_f1" if num_labels > 2 else "f1",
        greater_is_better=True,
        logging_steps=25,
        save_total_limit=2,
        bf16=(dtype == torch.bfloat16),
        fp16=(dtype == torch.float16),
        warmup_ratio=0.05,
        weight_decay=0.01,
        dataloader_num_workers=2,
        report_to="none",
        dataloader_pin_memory=torch.cuda.is_available(),
    )

    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)

    # Wrap metrics so they match Trainer's signature (p) but keep access to id2label
    if num_labels > 2:

        def compute_metrics_fn(p, _id2label=id2label):
            return multiclass_metrics((p.predictions, p.label_ids), _id2label)

    else:

        def compute_metrics_fn(p):
            return binary_metrics((p.predictions, p.label_ids))

    trainer = WeightedTrainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        processing_class=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics_fn,
        class_weights=class_weights,
    )

    trainer.train()
    eval_out = trainer.evaluate()

    # Save metrics
    os.makedirs(output_dir, exist_ok=True)
    with open(
        os.path.join(output_dir, f"{task_name}_metrics.json"), "w", encoding="utf-8"
    ) as f:
        json.dump(eval_out, f, indent=2)

    # Save label maps for downstream inference
    with open(os.path.join(output_dir, "label_maps.json"), "w", encoding="utf-8") as f:
        json.dump({"id2label": id2label, "label2id": label2id}, f, indent=2)

    return eval_out, tokenizer


# -----------------------------
# Main
# -----------------------------


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", type=str, required=True, help="Path to emails CSV.")
    parser.add_argument(
        "--text-col",
        type=str,
        default="text",
        help="Name of the text column to use (default: text).",
    )
    parser.add_argument("--model-name", type=str, default="answerdotai/ModernBERT-base")
    parser.add_argument(
        "--max-length",
        type=int,
        default=1024,
        help="Truncation length (ModernBERT supports up to 8192).",
    )
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--batch-size", type=int, default=8)
    parser.add_argument(
        "--grad-accum", type=int, default=4, help="Gradient accumulation steps."
    )
    parser.add_argument("--lr", type=float, default=2e-5)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument(
        "--no-weighted-loss", action="store_true", help="Disable class-weighted loss."
    )
    parser.add_argument("--output-root", type=str, default="outputs")
    args = parser.parse_args()

    if torch.cuda.is_available():
        # Free speed on Ampere for matmul-heavy ops
        torch.set_float32_matmul_precision("high")

    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    # Load CSV
    df = pd.read_csv(args.csv)
    texts = prepare_text_column(df, args.text_col)
    # Drop/clean labels
    df["category"] = clean_labels(df, "category")
    df["important"] = clean_labels(df, "important")

    # -------------------------------------
    # Task 1: Category (multiclass)
    # -------------------------------------
    # Build label maps
    cat_labels_sorted = sorted(df["category"].astype(str).unique())
    label2id_cat = {lab: i for i, lab in enumerate(cat_labels_sorted)}
    id2label_cat = {i: lab for lab, i in label2id_cat.items()}
    y_cat = df["category"].map(label2id_cat).tolist()

    X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(
        texts.tolist(), y_cat, test_size=0.15, random_state=args.seed, stratify=y_cat
    )

    out_cat = os.path.join(args.output_root, "modernbert_category")
    metrics_cat, _ = train_one_task(
        model_name=args.model_name,
        train_texts=X_train_c,
        train_labels=y_train_c,
        val_texts=X_val_c,
        val_labels=y_val_c,
        output_dir=out_cat,
        task_name="category",
        id2label=id2label_cat,
        label2id=label2id_cat,
        max_length=args.max_length,
        lr=args.lr,
        batch_size=args.batch_size,
        grad_accum=args.grad_accum,
        epochs=args.epochs,
        use_weighted_loss=not args.no_weighted_loss,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        seed=args.seed,
    )

    # -------------------------------------
    # Task 2: Important (binary)
    # -------------------------------------
    y_imp = df["important"].astype(int).tolist()
    X_train_i, X_val_i, y_train_i, y_val_i = train_test_split(
        texts.tolist(), y_imp, test_size=0.15, random_state=args.seed, stratify=y_imp
    )

    id2label_imp = {0: "not_important", 1: "important"}
    label2id_imp = {"not_important": 0, "important": 1}

    out_imp = os.path.join(args.output_root, "modernbert_important")
    metrics_imp, _ = train_one_task(
        model_name=args.model_name,
        train_texts=X_train_i,
        train_labels=y_train_i,
        val_texts=X_val_i,
        val_labels=y_val_i,
        output_dir=out_imp,
        task_name="important",
        id2label=id2label_imp,
        label2id=label2id_imp,
        max_length=args.max_length,
        lr=args.lr,
        batch_size=args.batch_size,
        grad_accum=args.grad_accum,
        epochs=args.epochs,
        use_weighted_loss=not args.no_weighted_loss,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        seed=args.seed,
    )

    # Aggregate & print
    summary = {
        "category": metrics_cat,
        "important": metrics_imp,
        "output_dirs": {"category": out_cat, "important": out_imp},
        "model_name": args.model_name,
        "max_length": args.max_length,
    }
    print(json.dumps(summary, indent=2))


if __name__ == "__main__":
    main()
